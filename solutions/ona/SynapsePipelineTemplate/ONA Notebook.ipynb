{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "onasynapsepool",
              "session_id": "1",
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-09-26T08:25:49.2043804Z",
              "session_start_time": "2023-09-26T08:25:49.2819552Z",
              "execution_start_time": "2023-09-26T08:29:40.9022344Z",
              "execution_finish_time": "2023-09-26T08:29:41.0857876Z",
              "spark_jobs": null,
              "parent_msg_id": "038dd9fc-f0f2-4db2-8655-2c62cbf2cbcd"
            },
            "text/plain": "StatementMeta(onasynapsepool, 1, 2, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# Default parameters that can be freely changed or overriden by pipeline run \r\n",
        "\r\n",
        "# Inputs\r\n",
        "calendarPath = \"abfss://mgdc@onastoredjtgcd3wlt2vc.dfs.core.windows.net/calendar_2023-07-01_to_2023-08-01_json/\"\r\n",
        "emailPath = \"abfss://mgdc@onastoredjtgcd3wlt2vc.dfs.core.windows.net/email_2023-07-01_to_2023-08-01_json/\"\r\n",
        "teamsChatPath = \"abfss://mgdc@onastoredjtgcd3wlt2vc.dfs.core.windows.net/teamschat_2023-07-01_to_2023-08-01_json/\"\r\n",
        "userPath = \"abfss://mgdc@onastoredjtgcd3wlt2vc.dfs.core.windows.net/user_2023-07-01_to_2023-08-01_json/\"\r\n",
        "\r\n",
        "#Output Format: Can be csv or parquet\r\n",
        "outputFormat = \"csv\"\r\n",
        "\r\n",
        "# Output path of user vertices\r\n",
        "usersOutputPath = \"abfss://output@onastoredjtgcd3wlt2vc.dfs.core.windows.net/users_2023-07-01_to_2023-08-01.csv\"\r\n",
        "\r\n",
        "# Output path of user to user edges\r\n",
        "interactionsOutputPath = \"abfss://output@onastoredjtgcd3wlt2vc.dfs.core.windows.net/interactions_2023-07-01_to_2023-08-01.csv\"\r\n",
        "\r\n",
        "# StartDate/EndDate for this run that is denormalized to users and interactions tables\r\n",
        "period = \"2022-06-01_to_2022-06-30\"\r\n",
        "\r\n",
        "# Whether or not to md5 hash the input user emails\r\n",
        "obfuscateEmails = True\r\n",
        "\r\n",
        "# Whether the input MGDC data is parquet (True) or json (False)\r\n",
        "isParquetInput = False\r\n",
        "\r\n",
        "# Leiden max cluster size, the maximum possible size for a detected community\r\n",
        "leidenMaxClusterSize = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "onasynapsepool",
              "session_id": "1",
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-09-26T08:29:18.7906269Z",
              "session_start_time": null,
              "execution_start_time": "2023-09-26T08:29:41.2667498Z",
              "execution_finish_time": "2023-09-26T08:29:41.4424171Z",
              "spark_jobs": null,
              "parent_msg_id": "626a3205-6e2e-4a2a-a7b8-25a3b6701123"
            },
            "text/plain": "StatementMeta(onasynapsepool, 1, 3, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.functions import coalesce, col, count, explode, format_number, isnull, lit, md5, rand, size, udf, unix_timestamp\r\n",
        "import pyspark.sql.functions as F\r\n",
        "from pyspark.sql.types import ArrayType, BooleanType, StringType, StructField, StructType\r\n",
        "from pyspark.sql import types as t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "onasynapsepool",
              "session_id": "1",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-09-26T08:37:22.6982271Z",
              "session_start_time": null,
              "execution_start_time": "2023-09-26T08:37:22.8618498Z",
              "execution_finish_time": "2023-09-26T08:37:24.7508578Z",
              "spark_jobs": null,
              "parent_msg_id": "0f39a543-99c1-400a-a582-5ae3faac8c97"
            },
            "text/plain": "StatementMeta(onasynapsepool, 1, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "microsoft": {},
        "collapsed": false
      },
      "source": [
        "# Load data\r\n",
        "areEmailsLoaded = False\r\n",
        "areMeetingsLoaded = False\r\n",
        "areTeamsChatsLoaded = False\r\n",
        "\r\n",
        "try:\r\n",
        "    if isParquetInput == True:\r\n",
        "        emailsRaw = spark.read.parquet(emailPath).select(\"internetMessageId\", \"Sender\", \"ToRecipients\")\r\n",
        "    else:\r\n",
        "        emailsRaw = spark.read.json(emailPath)\r\n",
        "    areEmailsLoaded = True\r\n",
        "except (Exception) as error:\r\n",
        "    print(error)\r\n",
        "    print(\"Emails data not loaded, continuing with empty emails\")\r\n",
        "    emailsSchema = StructType([StructField(\"internetMessageId\",StringType(),True),StructField(\"Sender\",StructType([StructField(\"emailAddress\",StructType([StructField(\"address\",StringType(),True),StructField(\"name\",StringType(),True)]),True)]),True),StructField(\"ToRecipients\",ArrayType(StructType([StructField(\"emailAddress\",StructType([StructField(\"address\",StringType(),True),StructField(\"name\",StringType(),True)]),True)]),True),True)])\r\n",
        "    emailsRaw = spark.createDataFrame(sc.emptyRDD(), emailsSchema)\r\n",
        "try:\r\n",
        "    if isParquetInput == True:\r\n",
        "        meetingsRaw = spark.read.parquet(calendarPath).select(\"iCalUId\", \"organizer\", \"attendees\", \"start\", \"end\", \"isAllDay\", \"isCancelled\")\r\n",
        "    else:\r\n",
        "        meetingsRaw = spark.read.json(calendarPath)\r\n",
        "    areMeetingsLoaded = True\r\n",
        "except (Exception) as error:\r\n",
        "    print(error)\r\n",
        "    print(\"Calendar data not loaded, continuing with empty meetings\")\r\n",
        "    meetingsSchema = StructType([StructField(\"iCalUId\",StringType(),True),StructField(\"organizer\",StructType([StructField(\"emailAddress\",StructType([StructField(\"address\",StringType(),True),StructField(\"name\",StringType(),True)]),True)]),True),StructField(\"attendees\",ArrayType(StructType([StructField(\"emailAddress\",StructType([StructField(\"address\",StringType(),True),StructField(\"name\",StringType(),True)]),True),StructField(\"status\",StructType([StructField(\"response\",StringType(),True),StructField(\"time\",StringType(),True)]),True),StructField(\"type\",StringType(),True)]),True),True),StructField(\"start\",StructType([StructField(\"dateTime\",StringType(),True),StructField(\"timeZone\",StringType(),True)]),True),StructField(\"end\",StructType([StructField(\"dateTime\",StringType(),True),StructField(\"timeZone\",StringType(),True)]),True),StructField(\"isAllDay\",BooleanType(),True),StructField(\"isCancelled\",BooleanType(),True)])\r\n",
        "    meetingsRaw = spark.createDataFrame(sc.emptyRDD(), meetingsSchema)\r\n",
        "try:\r\n",
        "    if isParquetInput == True:\r\n",
        "        teamschatsRaw = spark.read.parquet(teamsChatPath).select(\"internetMessageId\", \"Sender\", \"ToRecipients\")\r\n",
        "    else:\r\n",
        "        teamschatsRaw = spark.read.json(teamsChatPath)\r\n",
        "    areTeamsChatsLoaded = True\r\n",
        "except (Exception) as error:\r\n",
        "    print(error)\r\n",
        "    print(\"TeamsChats data not loaded, continuing with empty teams chats\")\r\n",
        "    teamschatsSchema = StructType([StructField(\"internetMessageId\",StringType(),True),StructField(\"Sender\",StructType([StructField(\"EmailAddress\",StructType([StructField(\"Address\",StringType(),True),StructField(\"Name\",StringType(),True)]),True)]),True),StructField(\"ToRecipients\",ArrayType(StructType([StructField(\"EmailAddress\",StructType([StructField(\"Address\",StringType(),True),StructField(\"Name\",StringType(),True)]),True)]),True),True)])\r\n",
        "    teamschatsRaw = spark.createDataFrame(sc.emptyRDD(), teamschatsSchema)\r\n",
        "\r\n",
        "if (not(areEmailsLoaded) and not(areMeetingsLoaded) and not(areTeamsChatsLoaded)):\r\n",
        "    raise Exception(\"No Emails, Meetings, or TeamsChats data loaded, unable to continue. Check the file paths.\")\r\n",
        "\r\n",
        "try:\r\n",
        "    if isParquetInput == True:\r\n",
        "        usersRaw = spark.read.parquet(userPath)\r\n",
        "    else:\r\n",
        "        usersRaw = spark.read.json(userPath)\r\n",
        "except (Exception) as error:\r\n",
        "    print(error)\r\n",
        "    raise Exception(\"Users data not loaded. Check the file path.\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "onasynapsepool",
              "session_id": "1",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-09-26T08:37:26.8818347Z",
              "session_start_time": null,
              "execution_start_time": "2023-09-26T08:37:27.0764938Z",
              "execution_finish_time": "2023-09-26T08:37:27.2581431Z",
              "spark_jobs": null,
              "parent_msg_id": "46588953-64cb-456d-86d9-f08e974aa2fc"
            },
            "text/plain": "StatementMeta(onasynapsepool, 1, 10, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Drop duplicates\r\n",
        "usersDedup = usersRaw.dropDuplicates([\"mail\"]).where(col(\"mail\").isNotNull())\r\n",
        "emailsDedup = emailsRaw.dropDuplicates([\"InternetMessageId\"]).select(\"Sender\", \"ToRecipients\")\r\n",
        "teamschatsDedup = teamschatsRaw.dropDuplicates([\"InternetMessageId\"]).select(\"Sender\", \"ToRecipients\")\r\n",
        "meetingsDedup = meetingsRaw.dropDuplicates([\"iCalUId\"]).select(\"organizer\", \"attendees\", \"start\", \"end\", \"isAllDay\", \"isCancelled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "onasynapsepool",
              "session_id": "1",
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-09-26T08:37:30.6358619Z",
              "session_start_time": null,
              "execution_start_time": "2023-09-26T08:37:30.8107016Z",
              "execution_finish_time": "2023-09-26T08:37:31.3794637Z",
              "spark_jobs": null,
              "parent_msg_id": "17d4b06b-bb47-4c26-8efb-e3cd13b2b118"
            },
            "text/plain": "StatementMeta(onasynapsepool, 1, 11, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Get the user email addresses and filter emails, teamschat, and meetings to only contain edges with those users\r\n",
        "usersEmailAddresses = usersDedup.selectExpr(\"lower(mail) as id\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Explode row with one sender -> N recipients into N rows\r\n",
        "# Filter to only keep emails with 8 or less recipients\r\n",
        "emails = emailsDedup.where(size(col(\"ToRecipients\")) <= 8) \\\r\n",
        "                    .withColumn(\"weight\", 1.0/size(col(\"ToRecipients\"))) \\\r\n",
        "                    .select(F.lower(col(\"Sender.EmailAddress.Address\")).alias(\"sender\"), col(\"weight\"), explode(col(\"ToRecipients\")).alias(\"exploded\")) \\\r\n",
        "                    .join(usersEmailAddresses, col(\"id\") == col(\"sender\"), \"inner\").drop(\"id\") \\\r\n",
        "                    .join(usersEmailAddresses, col(\"id\") == F.lower(col(\"exploded.EmailAddress.Address\")), \"inner\").drop(\"id\") \\\r\n",
        "                    .withColumnRenamed(\"sender\", \"src\") \\\r\n",
        "                    .withColumn(\"dst\", F.lower(col(\"exploded.EmailAddress.Address\"))) \\\r\n",
        "                    .select(col(\"src\"), col(\"dst\"), col(\"weight\")) \\\r\n",
        "                    .where(col(\"src\") != col(\"dst\"))\r\n",
        "if obfuscateEmails:\r\n",
        "    emails = emails.withColumn(\"srcHash\", md5(col(\"src\"))) \\\r\n",
        "                .withColumn(\"dstHash\", md5(col(\"dst\"))) \\\r\n",
        "                .drop(\"src\", \"dst\").selectExpr(\"srcHash as src\", \"dstHash as dst\", \"weight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Explode row with one organizer -> N attendees into N rows\r\n",
        "# Filter to only keep meetings at least 2 and at most 9 attendees. (Number of attendees includes the organizer)\r\n",
        "# Filter out cancelled and all day meetings\r\n",
        "# Filter to the meeting instance belonging to the organizer's calendar\r\n",
        "# Weight by meeting length in seconds divided by 400 (6.67 minutes) and divided by the number of recipients\r\n",
        "\r\n",
        "dtFormat = \"yyyy-MM-dd'T'HH:mm:ss.SSSSSSS\"\r\n",
        "meetings = meetingsDedup.where((size(col(\"attendees\")) <= 9) & (size(col(\"attendees\")) >= 2)) \\\r\n",
        "                        .where((col(\"isAllDay\") == False) & (col(\"isCancelled\") == False)) \\\r\n",
        "                        .withColumn(\"meetingDurationInSeconds\", unix_timestamp(col(\"end.dateTime\"), dtFormat).cast(\"long\") - unix_timestamp(col(\"start.dateTime\"), dtFormat).cast(\"long\")) \\\r\n",
        "                        .withColumn(\"weight\", (col(\"meetingDurationInSeconds\")/400.0) / (size(col(\"attendees\")) - 1)) \\\r\n",
        "                        .select(F.lower(col(\"organizer.emailAddress.address\")).alias(\"sender\"), col(\"weight\"), col(\"meetingDurationInSeconds\"), explode(col(\"attendees\")).alias(\"exploded\")) \\\r\n",
        "                        .join(usersEmailAddresses, col(\"id\") == col(\"sender\"), \"inner\").drop(\"id\") \\\r\n",
        "                        .join(usersEmailAddresses, col(\"id\") == F.lower(col(\"exploded.EmailAddress.Address\")), \"inner\").drop(\"id\") \\\r\n",
        "                        .withColumnRenamed(\"sender\", \"src\") \\\r\n",
        "                        .withColumn(\"dst\", F.lower(col(\"exploded.EmailAddress.Address\"))) \\\r\n",
        "                        .select(col(\"src\"), col(\"dst\"), col(\"weight\"), col(\"meetingDurationInSeconds\")) \\\r\n",
        "                        .where(col(\"src\") != col(\"dst\"))\r\n",
        "if obfuscateEmails:\r\n",
        "    meetings = meetings.withColumn(\"srcHash\", md5(col(\"src\"))) \\\r\n",
        "                       .withColumn(\"dstHash\", md5(col(\"dst\"))) \\\r\n",
        "                       .drop(\"src\", \"dst\").selectExpr(\"srcHash as src\", \"dstHash as dst\", \"weight\", \"meetingDurationInSeconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Explode row with one sender -> N recipients into N rows\r\n",
        "# Filter to only keep teamschat messages with 8 or less recipients\r\n",
        "teamschats = teamschatsDedup.where(size(col(\"ToRecipients\")) <= 8) \\\r\n",
        "                            .withColumn(\"weight\", 1.0/(8*size(col(\"ToRecipients\")))) \\\r\n",
        "                            .select(F.lower(col(\"Sender.EmailAddress.Address\")).alias(\"sender\"), col(\"weight\"), explode(col(\"ToRecipients\")).alias(\"exploded\")) \\\r\n",
        "                            .join(usersEmailAddresses, col(\"id\") == col(\"sender\"), \"inner\").drop(\"id\") \\\r\n",
        "                            .join(usersEmailAddresses, col(\"id\") == F.lower(col(\"exploded.EmailAddress.Address\")), \"inner\").drop(\"id\") \\\r\n",
        "                            .withColumnRenamed(\"sender\", \"src\") \\\r\n",
        "                            .withColumn(\"dst\", F.lower(col(\"exploded.EmailAddress.Address\"))) \\\r\n",
        "                            .select(col(\"src\"), col(\"dst\"), col(\"weight\")) \\\r\n",
        "                            .where(col(\"src\") != col(\"dst\"))\r\n",
        "if obfuscateEmails:\r\n",
        "    teamschats = teamschats.withColumn(\"srcHash\", md5(col(\"src\"))) \\\r\n",
        "                           .withColumn(\"dstHash\", md5(col(\"dst\"))) \\\r\n",
        "                           .drop(\"src\", \"dst\").selectExpr(\"srcHash as src\", \"dstHash as dst\", \"weight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Join after counting and summing weights from emails, teams chats, and meetings\r\n",
        "emailEdges = emails.groupBy(\"src\", \"dst\").agg(F.count(col(\"dst\")).alias(\"InteractionsEmail\"), F.sum(col(\"weight\")).alias(\"EmailWeight\")) \\\r\n",
        "                    .withColumnRenamed(\"src\", \"src1\").withColumnRenamed(\"dst\", \"dst1\")\r\n",
        "\r\n",
        "meetingEdges = meetings.groupBy(\"src\", \"dst\").agg(F.count(col(\"dst\")).alias(\"InteractionsMeetings\"), F.sum(col(\"weight\")).alias(\"MeetingsWeight\")) \\\r\n",
        "                           .withColumnRenamed(\"src\", \"src2\").withColumnRenamed(\"dst\", \"dst2\")\r\n",
        "\r\n",
        "teamsChatEdges = teamschats.groupBy(\"src\", \"dst\").agg(F.count(col(\"dst\")).alias(\"InteractionsTeamsChat\"), F.sum(col(\"weight\")).alias(\"TeamsChatWeight\")) \\\r\n",
        "                           .withColumnRenamed(\"src\", \"src3\").withColumnRenamed(\"dst\", \"dst3\")\r\n",
        "\r\n",
        "allEdges = emailEdges.alias(\"e\").join(meetingEdges.alias(\"m\"), (col(\"src1\") == col(\"src2\")) & (col(\"dst1\") == col(\"dst2\")), \"full\") \\\r\n",
        "                                .join(teamsChatEdges.alias(\"t\"), (col(\"src1\") == col(\"src3\")) & (col(\"dst1\") == col(\"dst3\")), \"full\")\r\n",
        "                              "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Coalesce together src/dst duplicate columns after join\r\n",
        "teamsChatToEmailRatio = 8 # interaction ratio for teamschat to email\r\n",
        "edgesCombined = allEdges.select(\r\n",
        "    coalesce( *[col(c) for c in [\"src1\", \"src2\", \"src3\"]]).alias(\"Source\"),\r\n",
        "    coalesce( *[col(c) for c in [\"dst1\", \"dst2\", \"dst3\"]]).alias(\"Target\"),\r\n",
        "    col(\"InteractionsEmail\"),\r\n",
        "    col(\"InteractionsMeetings\"),\r\n",
        "    col(\"InteractionsTeamsChat\"),\r\n",
        "    col(\"EmailWeight\"),\r\n",
        "    col(\"MeetingsWeight\"),\r\n",
        "    col(\"TeamsChatWeight\")\r\n",
        "    ).fillna(0) \\\r\n",
        "    .withColumn(\"Interactions\", (col(\"InteractionsEmail\") + col(\"InteractionsMeetings\") + F.round(col(\"InteractionsTeamsChat\")/teamsChatToEmailRatio)).cast('int')) \\\r\n",
        "    .withColumn(\"InteractionsWeight\", (col(\"EmailWeight\") + col(\"MeetingsWeight\") + col(\"TeamsChatWeight\")/teamsChatToEmailRatio)) \\\r\n",
        "    .withColumn(\"Period\", lit(period))\r\n",
        "\r\n",
        "interactionsOutputPath = interactionsOutputPath.replace(\".csv\",\"\")\r\n",
        "\r\n",
        "if outputFormat == \"csv\":\r\n",
        "    edgesCombined.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(interactionsOutputPath)\r\n",
        "    \r\n",
        "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\r\n",
        "    # get the part file generated by spark write\r\n",
        "    fs = Path(interactionsOutputPath).getFileSystem(sc._jsc.hadoopConfiguration())\r\n",
        "    part_file = fs.globStatus(Path(interactionsOutputPath + \"/part*\"))[0].getPath()\r\n",
        "    # set final target path\r\n",
        "    target_path_interactions = interactionsOutputPath + \".\" + outputFormat\r\n",
        "    # move and rename the file\r\n",
        "    fs.delete(Path(target_path_interactions), True)\r\n",
        "    fs.rename(part_file, Path(target_path_interactions))\r\n",
        "    fs.delete(Path(interactionsOutputPath), True)\r\n",
        "elif outputFormat == \"parquet\":\r\n",
        "    edgesCombined.write.option(\"header\", True).mode(\"overwrite\").parquet(interactionsOutputPath)\r\n",
        "else:\r\n",
        "    raise Exception (\"outputFormat should be csv or parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "eventsOrganized = meetings.groupBy(\"src\").count().withColumnRenamed(\"count\", \"NumberOfEventsOrganized\")\r\n",
        "eventsAttended = meetings.groupBy(\"dst\").count().withColumnRenamed(\"count\", \"NumberOfEventsAttended\")\r\n",
        "emailsSent = emails.groupBy(\"src\").count().withColumnRenamed(\"count\", \"NumberOfEmailsSent\")\r\n",
        "emailsReceived = emails.groupBy(\"dst\").count().withColumnRenamed(\"count\", \"NumberOfEmailsReceived\")\r\n",
        "teamsChatsSent = teamschats.groupBy(\"src\").count().withColumnRenamed(\"count\", \"NumberOfChatsSent\")\r\n",
        "teamsChatsReceived = teamschats.groupBy(\"dst\").count().withColumnRenamed(\"count\", \"NumberOfChatsReceived\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Select user properties for output and join all raw email/teamschat/meeting counts\r\n",
        "if obfuscateEmails:\r\n",
        "    usersDedup = usersDedup.withColumn(\"EmailAddress\",  md5(F.lower(col(\"mail\"))))\r\n",
        "else:\r\n",
        "    usersDedup = usersDedup.withColumn(\"EmailAddress\", F.lower(col(\"mail\")))\r\n",
        "usersRenamed = usersDedup.selectExpr(\"EmailAddress\", \"department as Department\", \"jobTitle as Title\", \"state as StateOrProvince\",\r\n",
        "                                     \"country as Country\",\"preferredLanguage as Languages\")\r\n",
        "usersJoined = usersRenamed.join(eventsOrganized, col(\"src\") == col(\"EmailAddress\"), \"left\").drop(\"src\") \\\r\n",
        "                          .join(eventsAttended, col(\"dst\") == col(\"EmailAddress\"), \"left\").drop(\"dst\") \\\r\n",
        "                          .join(emailsSent, col(\"src\") == col(\"EmailAddress\"), \"left\").drop(\"src\") \\\r\n",
        "                          .join(emailsReceived, col(\"dst\") == col(\"EmailAddress\"), \"left\").drop(\"dst\") \\\r\n",
        "                          .join(teamsChatsSent, col(\"src\") == col(\"EmailAddress\"), \"left\").drop(\"src\") \\\r\n",
        "                          .join(teamsChatsReceived, col(\"dst\") == col(\"EmailAddress\"), \"left\").drop(\"dst\") \\\r\n",
        "                          .fillna(0)\r\n",
        "numUsers = usersJoined.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Calculate out-degrees and in-degrees based on number of connections\r\n",
        "outDegreeEdges = edgesCombined.where(col(\"Interactions\") > 0).groupBy(\"Source\").count().select(col(\"Source\"), col(\"count\").alias(\"Out-DegreeIndex\"))\r\n",
        "inDegreeEdges = edgesCombined.where(col(\"Interactions\") > 0).groupBy(\"Target\").count().select(col(\"Target\"), col(\"count\").alias(\"In-DegreeIndex\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Construct networkx graph object\r\n",
        "import networkx as nx\r\n",
        "edges = edgesCombined.selectExpr(\"Source as src\", \"Target as dst\", \"InteractionsWeight as wgt\") \\\r\n",
        "                     .where((col(\"InteractionsWeight\") >= 0.25) & (col(\"InteractionsWeight\") <= 2000))\r\n",
        "edgesList = [(e.src, e.dst, e.wgt) for e in edges.collect()]\r\n",
        "graph = nx.DiGraph()\r\n",
        "graph.add_weighted_edges_from(edgesList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Calculate Influence Index based on page rank\r\n",
        "graphPageRank = nx.pagerank(graph, alpha=0.85, personalization=None, max_iter=100, tol=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define udf for adding page rank to dataframe\r\n",
        "maxPageRank = max(graphPageRank.values())\r\n",
        "def getPageRank(x):\r\n",
        "    pageRank = graphPageRank.get(x)\r\n",
        "    if pageRank is None:\r\n",
        "        return 0\r\n",
        "    return  pageRank / maxPageRank\r\n",
        "influenceIndexUdf = udf(getPageRank, t.FloatType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Calculate Community Bridging Index based on Leiden community detection\r\n",
        "import graspologic\r\n",
        "from graspologic.partition import leiden\r\n",
        "\r\n",
        "# Constructs undirected graph using bidrectional edges only, see networkx DiGraph.to_undirected doc\r\n",
        "undirectedGraph = graph.to_undirected()\r\n",
        "\r\n",
        "leidenResult = graspologic.partition.hierarchical_leiden(undirectedGraph, max_cluster_size=leidenMaxClusterSize)\r\n",
        "leidenClusters = leidenResult.final_level_hierarchical_clustering()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Construct udf for mapping users to community label\r\n",
        "def getLabel(x):\r\n",
        "    return leidenClusters.get(x)\r\n",
        "\r\n",
        "labelUdf = udf(getLabel, t.StringType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Counts how many communities C a user is connected to with an out edge, normalized by num of communities\r\n",
        "# For all users, compute C / (num of Communities)\r\n",
        "# 1 = they are connected to all communities\r\n",
        "# 0 = they have no connections\r\n",
        "\r\n",
        "# enrich edges by mapping target dst node to community\r\n",
        "edgesLabelled = edges.withColumn(\"Community\", labelUdf(col(\"dst\"))).drop(\"dst\").distinct()\r\n",
        "\r\n",
        "# group on src and count how many distinct community labelled targets each src has\r\n",
        "numCommunities = len(set(leidenClusters.values()))\r\n",
        "communityBridging = edgesLabelled.groupBy(\"src\").count() \\\r\n",
        "                                 .withColumn(\"CommunityBridgeIndex\", col(\"count\") / float(numCommunities)).drop(\"count\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create interaction matrix from users to communities\r\n",
        "# Map edges from user to dstCommunity\r\n",
        "userToCommunityEdges = edges.withColumn(\"dstCommunity\", labelUdf(col(\"dst\"))).drop(\"dst\").toPandas()\r\n",
        "# Group all edges from same user to same dstCommunity\r\n",
        "userToCommunityEdges = edges.withColumn(\"dstCommunity\", labelUdf(col(\"dst\"))).drop(\"dst\")\r\n",
        "userToCommunityEdges = userToCommunityEdges.groupby(\"src\", \"dstCommunity\").agg(F.sum(\"wgt\").alias(\"weight\")).toPandas()\r\n",
        "interactionMatrix = userToCommunityEdges.pivot(index=\"src\", columns=\"dstCommunity\", values=\"weight\").fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# One-hot encode categorical features\r\n",
        "import pandas as pd\r\n",
        "categoricalFeatures = pd.get_dummies(usersJoined.selectExpr(\"EmailAddress as id\", \"Country\", \"Department\", \"Title\").toPandas(), columns=[\"Country\", \"Department\", \"Title\"], prefix=\"\", prefix_sep=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Join categorical features with interactions\r\n",
        "featureMatrix = pd.merge(categoricalFeatures, interactionMatrix, how='left', left_on=\"id\", right_on=\"src\").fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Use TSNE to compute coordinates\r\n",
        "from sklearn.manifold import TSNE\r\n",
        "scaleFactor = 100\r\n",
        "tsne = TSNE(n_components = 2)\r\n",
        "tsneResult = tsne.fit_transform(featureMatrix.drop(columns=\"id\")) * scaleFactor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create coordinates dataframe with user id as key\r\n",
        "featureMatrix[\"x\"] = tsneResult[:, 0]\r\n",
        "featureMatrix[\"y\"] = tsneResult[:, 1]\r\n",
        "coordinates = spark.createDataFrame(featureMatrix[[\"id\", \"x\", \"y\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Join all indexes to users and output\r\n",
        "usersEnriched = usersJoined.join(outDegreeEdges, col(\"Source\") == col(\"EmailAddress\"), \"left\").drop(\"Source\") \\\r\n",
        "                           .join(inDegreeEdges, col(\"Target\") == col(\"EmailAddress\"), \"left\").drop(\"Target\") \\\r\n",
        "                           .fillna(0) \\\r\n",
        "                           .withColumn(\"DegreeIndex\", (col(\"In-DegreeIndex\") + col(\"Out-DegreeIndex\")) / (2 * numUsers)) \\\r\n",
        "                           .withColumn(\"Community\", labelUdf(col(\"EmailAddress\"))) \\\r\n",
        "                           .join(communityBridging, col(\"src\") == col(\"EmailAddress\"), \"left\").drop(\"src\") \\\r\n",
        "                           .withColumn(\"InfluenceIndex\", influenceIndexUdf(col(\"EmailAddress\"))) \\\r\n",
        "                           .fillna(0) \\\r\n",
        "                           .withColumn(\"Period\", lit(period)) \\\r\n",
        "                           .join(coordinates, col(\"id\") == col(\"EmailAddress\")).drop(\"id\")\r\n",
        "\r\n",
        "usersOutputPath = usersOutputPath.replace(\".csv\",\"\")\r\n",
        "\r\n",
        "if outputFormat == \"csv\":\r\n",
        "    usersEnriched.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(usersOutputPath)\r\n",
        "    \r\n",
        "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\r\n",
        "    # get the part file generated by spark write\r\n",
        "    fs = Path(usersOutputPath).getFileSystem(sc._jsc.hadoopConfiguration())\r\n",
        "    part_file = fs.globStatus(Path(usersOutputPath + \"/part*\"))[0].getPath()\r\n",
        "    #set final target path\r\n",
        "    target_path_users = usersOutputPath + \".\" + outputFormat\r\n",
        "    # move and rename the file\r\n",
        "    fs.delete(Path(target_path_users), True)\r\n",
        "    fs.rename(part_file, Path(target_path_users))\r\n",
        "    fs.delete(Path(usersOutputPath), True)\r\n",
        "elif outputFormat == \"parquet\":\r\n",
        "    usersEnriched.write.option(\"header\", True).mode(\"overwrite\").parquet(usersOutputPath)\r\n",
        "else:\r\n",
        "    raise Exception (\"outputFormat should be csv or parquet\")"
      ]
    }
  ]
}