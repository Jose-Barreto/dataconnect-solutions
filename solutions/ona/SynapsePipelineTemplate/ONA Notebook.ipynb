{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Default parameters that can be freely changed or overriden by pipeline run \n",
        "\n",
        "# Inputs\n",
        "calendarPath = \"abfss://mgdc@onastoremzj5lge2pebi4.dfs.core.windows.net/calendar_2022-06-01_to_2022-07-01/\"\n",
        "emailPath = \"abfss://mgdc@onastoremzj5lge2pebi4.dfs.core.windows.net/email_2022-06-01_to_2022-07-01/\"\n",
        "teamsChatPath = \"abfss://mgdc@onastoremzj5lge2pebi4.dfs.core.windows.net/teamschat_2022-06-01_to_2022-07-01/\"\n",
        "userPath = \"abfss://mgdc@onastoremzj5lge2pebi4.dfs.core.windows.net/user_2022-06-01_to_2022-07-01/\"\n",
        "\n",
        "#Output Format: Can be csv or parquet\n",
        "outputFormat = \"csv\"\n",
        "\n",
        "# Output path of user vertices\n",
        "usersOutputPath = \"abfss://output@onastoremzj5lge2pebi4.dfs.core.windows.net/users_2022-06-01_to_2022-07-01.csv\"\n",
        "\n",
        "# Output path of user to user edges\n",
        "interactionsOutputPath = \"abfss://output@onastoremzj5lge2pebi4.dfs.core.windows.net/interactions_2022-06-01_to_2022-07-01.csv\"\n",
        "\n",
        "# StartDate/EndDate for this run that is denormalized to users and interactions tables\n",
        "period = \"2022-06-01 to 2022-07-01\"\n",
        "\n",
        "# Whether or not to md5 hash the input user emails\n",
        "obfuscateEmails = True\n",
        "\n",
        "# Whether the input MGDC data is parquet (True) or json (False)\n",
        "isParquetInput = True\n",
        "\n",
        "# Leiden max cluster size, the maximum possible size for a detected community\n",
        "leidenMaxClusterSize = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import coalesce, col, count, explode, format_number, isnull, lit, md5, rand, size, udf, unix_timestamp\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import ArrayType, BooleanType, StringType, StructField, StructType\n",
        "from pyspark.sql import types as t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "microsoft": {}
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "areEmailsLoaded = False\n",
        "areMeetingsLoaded = False\n",
        "areTeamsChatsLoaded = False\n",
        "\n",
        "try:\n",
        "    if isParquetInput == True:\n",
        "        emailsRaw = spark.read.parquet(emailPath).select(\"Id\", \"Sender\", \"ToRecipients\")\n",
        "    else:\n",
        "        emailsRaw = spark.read.json(emailPath)\n",
        "    areEmailsLoaded = True\n",
        "except (Exception) as error:\n",
        "    print(error)\n",
        "    print(\"Emails data not loaded, continuing with empty emails\")\n",
        "    emailsSchema = StructType([StructField(\"Id\",StringType(),True),StructField(\"Sender\",StructType([StructField(\"emailAddress\",StructType([StructField(\"address\",StringType(),True),StructField(\"name\",StringType(),True)]),True)]),True),StructField(\"ToRecipients\",ArrayType(StructType([StructField(\"emailAddress\",StructType([StructField(\"address\",StringType(),True),StructField(\"name\",StringType(),True)]),True)]),True),True)])\n",
        "    emailsRaw = spark.createDataFrame(sc.emptyRDD(), emailsSchema)\n",
        "try:\n",
        "    # Calendar dataset currently must be loaded as json until MDF bugfix rolls out week of 2023-05-15\n",
        "    meetingsRaw = spark.read.json(calendarPath)\n",
        "    areMeetingsLoaded = True\n",
        "except (Exception) as error:\n",
        "    print(error)\n",
        "    print(\"Calendar data not loaded, continuing with empty meetings\")\n",
        "    meetingsSchema = StructType([StructField(\"Id\",StringType(),True),StructField(\"organizer\",StructType([StructField(\"emailAddress\",StructType([StructField(\"address\",StringType(),True),StructField(\"name\",StringType(),True)]),True)]),True),StructField(\"attendees\",ArrayType(StructType([StructField(\"emailAddress\",StructType([StructField(\"address\",StringType(),True),StructField(\"name\",StringType(),True)]),True),StructField(\"status\",StructType([StructField(\"response\",StringType(),True),StructField(\"time\",StringType(),True)]),True),StructField(\"type\",StringType(),True)]),True),True),StructField(\"start\",StructType([StructField(\"dateTime\",StringType(),True),StructField(\"timeZone\",StringType(),True)]),True),StructField(\"end\",StructType([StructField(\"dateTime\",StringType(),True),StructField(\"timeZone\",StringType(),True)]),True),StructField(\"isAllDay\",BooleanType(),True),StructField(\"isCancelled\",BooleanType(),True),StructField(\"isOrganizer\",BooleanType(),True),StructField(\"iCalUId\",StringType(),True)])\n",
        "    meetingsRaw = spark.createDataFrame(sc.emptyRDD(), meetingsSchema)\n",
        "try:\n",
        "    if isParquetInput == True:\n",
        "        teamschatsRaw = spark.read.parquet(teamsChatPath).select(\"Id\", \"Sender\", \"ToRecipients\")\n",
        "    else:\n",
        "        teamschatsRaw = spark.read.json(teamsChatPath)\n",
        "    areTeamsChatsLoaded = True\n",
        "except (Exception) as error:\n",
        "    print(error)\n",
        "    print(\"TeamsChats data not loaded, continuing with empty teams chats\")\n",
        "    teamschatsSchema = StructType([StructField(\"Id\",StringType(),True),StructField(\"Sender\",StructType([StructField(\"EmailAddress\",StructType([StructField(\"Address\",StringType(),True),StructField(\"Name\",StringType(),True)]),True)]),True),StructField(\"ToRecipients\",ArrayType(StructType([StructField(\"EmailAddress\",StructType([StructField(\"Address\",StringType(),True),StructField(\"Name\",StringType(),True)]),True)]),True),True)])\n",
        "    teamschatsRaw = spark.createDataFrame(sc.emptyRDD(), teamschatsSchema)\n",
        "\n",
        "if (not(areEmailsLoaded) and not(areMeetingsLoaded) and not(areTeamsChatsLoaded)):\n",
        "    raise Exception(\"No Emails, Meetings, or TeamsChats data loaded, unable to continue. Check the file paths.\")\n",
        "\n",
        "try:\n",
        "    if isParquetInput == True:\n",
        "        usersRaw = spark.read.parquet(userPath)\n",
        "    else:\n",
        "        usersRaw = spark.read.json(userPath)\n",
        "except (Exception) as error:\n",
        "    print(error)\n",
        "    raise Exception(\"Users data not loaded. Check the file path.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Drop duplicates\n",
        "usersDedup = usersRaw.dropDuplicates([\"mail\"]).where(col(\"mail\").isNotNull())\n",
        "emailsDedup = emailsRaw.dropDuplicates([\"Id\"]).select(\"Sender\", \"ToRecipients\")\n",
        "teamschatsDedup = teamschatsRaw.dropDuplicates([\"Id\"]).select(\"Sender\", \"ToRecipients\")\n",
        "meetingsDedup = meetingsRaw.dropDuplicates([\"Id\"]).select(\"organizer\", \"attendees\", \"start\", \"end\", \"isAllDay\", \"isCancelled\", \"isOrganizer\", \"iCalUId\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get the user email addresses and filter emails, teamschat, and meetings to only contain edges with those users\n",
        "usersEmailAddresses = usersDedup.selectExpr(\"lower(mail) as id\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Explode row with one sender -> N recipients into N rows\n",
        "# Filter to only keep emails with 8 or less recipients\n",
        "emails = emailsDedup.where(size(col(\"ToRecipients\")) <= 8) \\\n",
        "                    .withColumn(\"weight\", 1.0/size(col(\"ToRecipients\"))) \\\n",
        "                    .select(F.lower(col(\"Sender.EmailAddress.Address\")).alias(\"sender\"), col(\"weight\"), explode(col(\"ToRecipients\")).alias(\"exploded\")) \\\n",
        "                    .join(usersEmailAddresses, col(\"id\") == col(\"sender\"), \"inner\").drop(\"id\") \\\n",
        "                    .join(usersEmailAddresses, col(\"id\") == F.lower(col(\"exploded.EmailAddress.Address\")), \"inner\").drop(\"id\") \\\n",
        "                    .withColumnRenamed(\"sender\", \"src\") \\\n",
        "                    .withColumn(\"dst\", F.lower(col(\"exploded.EmailAddress.Address\"))) \\\n",
        "                    .select(col(\"src\"), col(\"dst\"), col(\"weight\")) \\\n",
        "                    .where(col(\"src\") != col(\"dst\"))\n",
        "if obfuscateEmails:\n",
        "    emails = emails.withColumn(\"srcHash\", md5(col(\"src\"))) \\\n",
        "                .withColumn(\"dstHash\", md5(col(\"dst\"))) \\\n",
        "                .drop(\"src\", \"dst\").selectExpr(\"srcHash as src\", \"dstHash as dst\", \"weight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Explode row with one organizer -> N attendees into N rows\n",
        "# Filter to only keep meetings at least 2 and at most 9 attendees. (Number of attendees includes the organizer)\n",
        "# Filter out cancelled and all day meetings\n",
        "# Filter to the meeting instance belonging to the organizer's calendar\n",
        "# Weight by meeting length in seconds divided by 400 (6.67 minutes) and divided by the number of recipients\n",
        "\n",
        "dtFormat = \"yyyy-MM-dd'T'HH:mm:ss.SSSSSSS\"\n",
        "meetings = meetingsDedup.where((size(col(\"attendees\")) <= 9) & (size(col(\"attendees\")) >= 2)) \\\n",
        "                        .where((col(\"isAllDay\") == False) & (col(\"isCancelled\") == False) & (col(\"isOrganizer\") == True)) \\\n",
        "                        .withColumn(\"meetingDurationInSeconds\", unix_timestamp(col(\"end.dateTime\"), dtFormat).cast(\"long\") - unix_timestamp(col(\"start.dateTime\"), dtFormat).cast(\"long\")) \\\n",
        "                        .withColumn(\"weight\", (col(\"meetingDurationInSeconds\")/400.0) / (size(col(\"attendees\")) - 1)) \\\n",
        "                        .select(F.lower(col(\"organizer.emailAddress.address\")).alias(\"sender\"), col(\"weight\"), col(\"meetingDurationInSeconds\"), col(\"attendees\"), col(\"iCalUId\"), explode(col(\"attendees\")).alias(\"exploded\")) \\\n",
        "                        .join(usersEmailAddresses, col(\"id\") == col(\"sender\"), \"inner\").drop(\"id\") \\\n",
        "                        .join(usersEmailAddresses, col(\"id\") == F.lower(col(\"exploded.EmailAddress.Address\")), \"inner\").drop(\"id\") \\\n",
        "                        .withColumnRenamed(\"sender\", \"src\") \\\n",
        "                        .withColumn(\"dst\", F.lower(col(\"exploded.EmailAddress.Address\"))) \\\n",
        "                        .select(col(\"src\"), col(\"dst\"), col(\"weight\"), col(\"meetingDurationInSeconds\"), col(\"iCalUId\"), col(\"attendees\")) \\\n",
        "                        .where(col(\"src\") != col(\"dst\"))\n",
        "if obfuscateEmails:\n",
        "    meetings = meetings.withColumn(\"srcHash\", md5(col(\"src\"))) \\\n",
        "                       .withColumn(\"dstHash\", md5(col(\"dst\"))) \\\n",
        "                       .drop(\"src\", \"dst\").selectExpr(\"srcHash as src\", \"dstHash as dst\", \"weight\", \"meetingDurationInSeconds\", \"iCalUId\",\"attendees\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Explode row with one sender -> N recipients into N rows\n",
        "# Filter to only keep teamschat messages with 8 or less recipients\n",
        "teamschats = teamschatsDedup.where(size(col(\"ToRecipients\")) <= 8) \\\n",
        "                            .withColumn(\"weight\", 1.0/(8*size(col(\"ToRecipients\")))) \\\n",
        "                            .select(F.lower(col(\"Sender.EmailAddress.Address\")).alias(\"sender\"), col(\"weight\"), explode(col(\"ToRecipients\")).alias(\"exploded\")) \\\n",
        "                            .join(usersEmailAddresses, col(\"id\") == col(\"sender\"), \"inner\").drop(\"id\") \\\n",
        "                            .join(usersEmailAddresses, col(\"id\") == F.lower(col(\"exploded.EmailAddress.Address\")), \"inner\").drop(\"id\") \\\n",
        "                            .withColumnRenamed(\"sender\", \"src\") \\\n",
        "                            .withColumn(\"dst\", F.lower(col(\"exploded.EmailAddress.Address\"))) \\\n",
        "                            .select(col(\"src\"), col(\"dst\"), col(\"weight\")) \\\n",
        "                            .where(col(\"src\") != col(\"dst\"))\n",
        "if obfuscateEmails:\n",
        "    teamschats = teamschats.withColumn(\"srcHash\", md5(col(\"src\"))) \\\n",
        "                           .withColumn(\"dstHash\", md5(col(\"dst\"))) \\\n",
        "                           .drop(\"src\", \"dst\").selectExpr(\"srcHash as src\", \"dstHash as dst\", \"weight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Join after counting and summing weights from emails, teams chats, and meetings\n",
        "emailEdges = emails.groupBy(\"src\", \"dst\").agg(F.count(col(\"dst\")).alias(\"InteractionsEmail\"), F.sum(col(\"weight\")).alias(\"EmailWeight\")) \\\n",
        "                    .withColumnRenamed(\"src\", \"src1\").withColumnRenamed(\"dst\", \"dst1\")\n",
        "\n",
        "meetingEdges = meetings.groupBy(\"src\", \"dst\").agg(F.count(col(\"dst\")).alias(\"InteractionsMeetings\"), F.sum(col(\"weight\")).alias(\"MeetingsWeight\")) \\\n",
        "                           .withColumnRenamed(\"src\", \"src2\").withColumnRenamed(\"dst\", \"dst2\")\n",
        "\n",
        "teamsChatEdges = teamschats.groupBy(\"src\", \"dst\").agg(F.count(col(\"dst\")).alias(\"InteractionsTeamsChat\"), F.sum(col(\"weight\")).alias(\"TeamsChatWeight\")) \\\n",
        "                           .withColumnRenamed(\"src\", \"src3\").withColumnRenamed(\"dst\", \"dst3\")\n",
        "\n",
        "allEdges = emailEdges.alias(\"e\").join(meetingEdges.alias(\"m\"), (col(\"src1\") == col(\"src2\")) & (col(\"dst1\") == col(\"dst2\")), \"full\") \\\n",
        "                                .join(teamsChatEdges.alias(\"t\"), (col(\"src1\") == col(\"src3\")) & (col(\"dst1\") == col(\"dst3\")), \"full\")\n",
        "                              "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Coalesce together src/dst duplicate columns after join\n",
        "teamsChatToEmailRatio = 8 # interaction ratio for teamschat to email\n",
        "edgesCombined = allEdges.select(\n",
        "    coalesce( *[col(c) for c in [\"src1\", \"src2\", \"src3\"]]).alias(\"Source\"),\n",
        "    coalesce( *[col(c) for c in [\"dst1\", \"dst2\", \"dst3\"]]).alias(\"Target\"),\n",
        "    col(\"InteractionsEmail\"),\n",
        "    col(\"InteractionsMeetings\"),\n",
        "    col(\"InteractionsTeamsChat\"),\n",
        "    col(\"EmailWeight\"),\n",
        "    col(\"MeetingsWeight\"),\n",
        "    col(\"TeamsChatWeight\")\n",
        "    ).fillna(0) \\\n",
        "    .withColumn(\"Interactions\", (col(\"InteractionsEmail\") + col(\"InteractionsMeetings\") + F.round(col(\"InteractionsTeamsChat\")/teamsChatToEmailRatio)).cast('int')) \\\n",
        "    .withColumn(\"InteractionsWeight\", (col(\"EmailWeight\") + col(\"MeetingsWeight\") + col(\"TeamsChatWeight\")/teamsChatToEmailRatio)) \\\n",
        "    .withColumn(\"Period\", lit(period))\n",
        "\n",
        "interactionsOutputPath = interactionsOutputPath.replace(\".csv\",\"\")\n",
        "\n",
        "if outputFormat == \"csv\":\n",
        "    edgesCombined.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(interactionsOutputPath)\n",
        "    \n",
        "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
        "    # get the part file generated by spark write\n",
        "    fs = Path(interactionsOutputPath).getFileSystem(sc._jsc.hadoopConfiguration())\n",
        "    part_file = fs.globStatus(Path(interactionsOutputPath + \"/part*\"))[0].getPath()\n",
        "    # set final target path\n",
        "    target_path_interactions = interactionsOutputPath + \".\" + outputFormat\n",
        "    # move and rename the file\n",
        "    fs.delete(Path(target_path_interactions), True)\n",
        "    fs.rename(part_file, Path(target_path_interactions))\n",
        "    fs.delete(Path(interactionsOutputPath), True)\n",
        "elif outputFormat == \"parquet\":\n",
        "    edgesCombined.write.option(\"header\", True).mode(\"overwrite\").parquet(interactionsOutputPath)\n",
        "else:\n",
        "    raise Exception (\"outputFormat should be csv or parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "eventsOrganized = meetings.groupBy(\"src\").count().withColumnRenamed(\"count\", \"NumberOfEventsOrganized\")\n",
        "eventsAttended = meetings.groupBy(\"dst\").count().withColumnRenamed(\"count\", \"NumberOfEventsAttended\")\n",
        "emailsSent = emails.groupBy(\"src\").count().withColumnRenamed(\"count\", \"NumberOfEmailsSent\")\n",
        "emailsReceived = emails.groupBy(\"dst\").count().withColumnRenamed(\"count\", \"NumberOfEmailsReceived\")\n",
        "teamsChatsSent = teamschats.groupBy(\"src\").count().withColumnRenamed(\"count\", \"NumberOfChatsSent\")\n",
        "teamsChatsReceived = teamschats.groupBy(\"dst\").count().withColumnRenamed(\"count\", \"NumberOfChatsReceived\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Select user properties for output and join all raw email/teamschat/meeting counts\n",
        "if obfuscateEmails:\n",
        "    usersDedup = usersDedup.withColumn(\"EmailAddress\",  md5(F.lower(col(\"mail\"))))\n",
        "else:\n",
        "    usersDedup = usersDedup.withColumn(\"EmailAddress\", F.lower(col(\"mail\")))\n",
        "usersRenamed = usersDedup.selectExpr(\"EmailAddress\", \"department as Department\", \"jobTitle as Title\", \"state as StateOrProvince\",\n",
        "                                     \"country as Country\",\"preferredLanguage as Languages\")\n",
        "usersJoined = usersRenamed.join(eventsOrganized, col(\"src\") == col(\"EmailAddress\"), \"left\").drop(\"src\") \\\n",
        "                          .join(eventsAttended, col(\"dst\") == col(\"EmailAddress\"), \"left\").drop(\"dst\") \\\n",
        "                          .join(emailsSent, col(\"src\") == col(\"EmailAddress\"), \"left\").drop(\"src\") \\\n",
        "                          .join(emailsReceived, col(\"dst\") == col(\"EmailAddress\"), \"left\").drop(\"dst\") \\\n",
        "                          .join(teamsChatsSent, col(\"src\") == col(\"EmailAddress\"), \"left\").drop(\"src\") \\\n",
        "                          .join(teamsChatsReceived, col(\"dst\") == col(\"EmailAddress\"), \"left\").drop(\"dst\") \\\n",
        "                          .fillna(0)\n",
        "numUsers = usersJoined.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Calculate out-degrees and in-degrees based on number of connections\n",
        "outDegreeEdges = edgesCombined.where(col(\"Interactions\") > 0).groupBy(\"Source\").count().select(col(\"Source\"), col(\"count\").alias(\"Out-DegreeIndex\"))\n",
        "inDegreeEdges = edgesCombined.where(col(\"Interactions\") > 0).groupBy(\"Target\").count().select(col(\"Target\"), col(\"count\").alias(\"In-DegreeIndex\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Construct networkx graph object\n",
        "import networkx as nx\n",
        "edges = edgesCombined.selectExpr(\"Source as src\", \"Target as dst\", \"InteractionsWeight as wgt\") \\\n",
        "                     .where((col(\"InteractionsWeight\") >= 0.25) & (col(\"InteractionsWeight\") <= 2000))\n",
        "edgesList = [(e.src, e.dst, e.wgt) for e in edges.collect()]\n",
        "graph = nx.DiGraph()\n",
        "graph.add_weighted_edges_from(edgesList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Calculate Influence Index based on page rank\n",
        "graphPageRank = nx.pagerank(graph, alpha=0.85, personalization=None, max_iter=100, tol=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define udf for adding page rank to dataframe\n",
        "maxPageRank = max(graphPageRank.values())\n",
        "def getPageRank(x):\n",
        "    pageRank = graphPageRank.get(x)\n",
        "    if pageRank is None:\n",
        "        return 0\n",
        "    return  pageRank / maxPageRank\n",
        "influenceIndexUdf = udf(getPageRank, t.FloatType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Calculate Betweeness Index\n",
        "# Commented out since the complexity is O(EV) where E = edges, V = vertices\n",
        "# This will be slow for larger graphs, roughly above 10K users\n",
        "# graphBetweenness = nx.betweenness_centrality(graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define udf for adding betweeness to dataframe\n",
        "# def getBetweeness(x):\n",
        "#     return graphBetweenness.get(x)\n",
        "# betweenessIndexUdf = udf(getBetweeness, t.FloatType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Calculate Community Bridging Index based on Leiden community detection\n",
        "import graspologic\n",
        "from graspologic.partition import leiden\n",
        "\n",
        "# Constructs undirected graph using bidrectional edges only, see networkx DiGraph.to_undirected doc\n",
        "undirectedGraph = graph.to_undirected()\n",
        "\n",
        "leidenResult = graspologic.partition.hierarchical_leiden(undirectedGraph, max_cluster_size=leidenMaxClusterSize)\n",
        "leidenClusters = leidenResult.final_level_hierarchical_clustering()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Construct udf for mapping users to community label\n",
        "def getLabel(x):\n",
        "    return leidenClusters.get(x)\n",
        "\n",
        "labelUdf = udf(getLabel, t.StringType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Counts how many communities C a user is connected to with an out edge, normalized by num of communities\n",
        "# For all users, compute C / (num of Communities)\n",
        "# 1 = they are connected to all communities\n",
        "# 0 = they have no connections\n",
        "\n",
        "# enrich edges by mapping target dst node to community\n",
        "edgesLabelled = edges.withColumn(\"Community\", labelUdf(col(\"dst\"))).drop(\"dst\").distinct()\n",
        "\n",
        "# group on src and count how many distinct community labelled targets each src has\n",
        "numCommunities = len(set(leidenClusters.values()))\n",
        "communityBridging = edgesLabelled.groupBy(\"src\").count() \\\n",
        "                                 .withColumn(\"CommunityBridgeIndex\", col(\"count\") / float(numCommunities)).drop(\"count\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create interaction matrix from users to communities\n",
        "# Map edges from user to dstCommunity\n",
        "userToCommunityEdges = edges.withColumn(\"dstCommunity\", labelUdf(col(\"dst\"))).drop(\"dst\").toPandas()\n",
        "# Group all edges from same user to same dstCommunity\n",
        "userToCommunityEdges = edges.withColumn(\"dstCommunity\", labelUdf(col(\"dst\"))).drop(\"dst\")\n",
        "userToCommunityEdges = userToCommunityEdges.groupby(\"src\", \"dstCommunity\").agg(F.sum(\"wgt\").alias(\"weight\")).toPandas()\n",
        "interactionMatrix = userToCommunityEdges.pivot(index=\"src\", columns=\"dstCommunity\", values=\"weight\").fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# One-hot encode categorical features\n",
        "categoricalFeatures = pd.get_dummies(usersJoined.selectExpr(\"EmailAddress as id\", \"Country\", \"Department\", \"Title\").toPandas(), columns=[\"Country\", \"Department\", \"Title\"], prefix=\"\", prefix_sep=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Join categorical features with interactions\n",
        "featureMatrix = pd.merge(categoricalFeatures, interactionMatrix, how='left', left_on=\"id\", right_on=\"src\").fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Use TSNE to compute coordinates\n",
        "scaleFactor = 100\n",
        "tsne = TSNE(n_components = 2)\n",
        "tsneResult = tsne.fit_transform(featureMatrix.drop(columns=\"id\")) * scaleFactor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create coordinates dataframe with user id as key\n",
        "featureMatrix[\"x\"] = tsneResult[:, 0]\n",
        "featureMatrix[\"y\"] = tsneResult[:, 1]\n",
        "coordinates = spark.createDataFrame(featureMatrix[[\"id\", \"x\", \"y\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Join all indexes to users and output\n",
        "usersEnriched = usersJoined.join(outDegreeEdges, col(\"Source\") == col(\"EmailAddress\"), \"left\").drop(\"Source\") \\\n",
        "                           .join(inDegreeEdges, col(\"Target\") == col(\"EmailAddress\"), \"left\").drop(\"Target\") \\\n",
        "                           .fillna(0) \\\n",
        "                           .withColumn(\"DegreeIndex\", (col(\"In-DegreeIndex\") + col(\"Out-DegreeIndex\")) / (2 * numUsers)) \\\n",
        "                           .withColumn(\"Community\", labelUdf(col(\"EmailAddress\"))) \\\n",
        "                           .join(communityBridging, col(\"src\") == col(\"EmailAddress\"), \"left\").drop(\"src\") \\\n",
        "                           .withColumn(\"InfluenceIndex\", influenceIndexUdf(col(\"EmailAddress\"))) \\\n",
        "                           .fillna(0) \\\n",
        "                           .withColumn(\"Period\", lit(period)) \\\n",
        "                           .join(coordinates, col(\"id\") == col(\"EmailAddress\")).drop(\"id\")\n",
        "\n",
        "usersOutputPath = usersOutputPath.replace(\".csv\",\"\")\n",
        "\n",
        "if outputFormat == \"csv\":\n",
        "    usersEnriched.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(usersOutputPath)\n",
        "    \n",
        "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
        "    # get the part file generated by spark write\n",
        "    fs = Path(usersOutputPath).getFileSystem(sc._jsc.hadoopConfiguration())\n",
        "    part_file = fs.globStatus(Path(usersOutputPath + \"/part*\"))[0].getPath()\n",
        "    #set final target path\n",
        "    target_path_users = usersOutputPath + \".\" + outputFormat\n",
        "    # move and rename the file\n",
        "    fs.delete(Path(target_path_users), True)\n",
        "    fs.rename(part_file, Path(target_path_users))\n",
        "    fs.delete(Path(usersOutputPath), True)\n",
        "elif outputFormat == \"parquet\":\n",
        "    usersEnriched.write.option(\"header\", True).mode(\"overwrite\").parquet(usersOutputPath)\n",
        "else:\n",
        "    raise Exception (\"outputFormat should be csv or parquet\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
